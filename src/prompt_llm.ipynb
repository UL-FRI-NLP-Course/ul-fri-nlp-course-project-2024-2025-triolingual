{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01847c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd138336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Biljana\\anaconda3\\envs\\rag_llm_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"microsoft/phi-1_5\"  #\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "Tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "LLM = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded on:\", next(LLM.parameters()).device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28bb3002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "pipe = hf_pipeline(\n",
    "    \"text-generation\",\n",
    "    model=LLM,\n",
    "    tokenizer=Tok,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.2,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "20d01183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Biljana\\anaconda3\\envs\\rag_llm_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Answer: Paris.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "\n",
    "response = pipe(\n",
    "    f\"Question: {question}\\nAnswer:\", \n",
    "    max_new_tokens=100\n",
    ")\n",
    "\n",
    "full_text = response[0][\"generated_text\"]\n",
    "lines = full_text.split(\"\\n\")\n",
    "\n",
    "answer_line = \"\"\n",
    "for line in lines:\n",
    "    if \"Answer:\" in line:\n",
    "        answer_line = line.split(\"Answer:\")[-1].strip()\n",
    "        break\n",
    "\n",
    "print(\"Model Answer:\", answer_line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3aae21",
   "metadata": {},
   "source": [
    "### Extract the graphs in human readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fe7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd8f871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../world_leaders_qa_dataset.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a21bafe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P102', 'P22', 'P40', 'P106', 'P35']\n"
     ]
    }
   ],
   "source": [
    "def extract_all_property_ids_from_dataset(dataset):\n",
    "    property_ids = set()\n",
    "    for item in dataset:\n",
    "        question = item.get(\"question\", \"\")\n",
    "        matches = re.findall(r'http://www\\.wikidata\\.org/prop/direct/(p\\d+)', question, flags=re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            property_ids.add(match.upper())\n",
    "        graph = item.get(\"graph\", {})\n",
    "        edges = graph.get(\"edges\", [])\n",
    "        for edge in edges:\n",
    "            rel_id = edge.get(\"relation_id\", \"\")\n",
    "            if rel_id.startswith(\"P\"):\n",
    "                property_ids.add(rel_id.upper())\n",
    "    return list(property_ids)\n",
    "\n",
    "print(extract_all_property_ids_from_dataset(data)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9e62634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q12757697', 'Q4461939', 'Q114371233', 'Q93923', 'Q1192712']\n"
     ]
    }
   ],
   "source": [
    "def extract_all_entity_ids_from_dataset(dataset):\n",
    "    entity_ids = set()\n",
    "    for item in dataset:\n",
    "        graph = item.get(\"graph\", {})\n",
    "        nodes = graph.get(\"nodes\", [])\n",
    "        edges = graph.get(\"edges\", [])\n",
    "        for node in nodes:\n",
    "            entity_ids.add(node[\"id\"])\n",
    "        for edge in edges:\n",
    "            entity_ids.add(edge[\"source\"])\n",
    "            entity_ids.add(edge[\"target\"])\n",
    "    return list(entity_ids)\n",
    "\n",
    "print(extract_all_entity_ids_from_dataset(data)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dedb457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_ids = extract_all_property_ids_from_dataset(data)\n",
    "entity_ids = extract_all_entity_ids_from_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e097f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get corresponing labels from wikidata\n",
    "def get_property_label(property_id):\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{property_id}.json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return property_id\n",
    "    try:\n",
    "        data = response.json()\n",
    "        return data[\"entities\"][property_id][\"labels\"][\"en\"][\"value\"]\n",
    "    except:\n",
    "        return property_id\n",
    "\n",
    "def get_entity_label(entity_id):\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return entity_id\n",
    "    try:\n",
    "        data = response.json()\n",
    "        return data[\"entities\"][entity_id][\"labels\"][\"en\"][\"value\"]\n",
    "    except:\n",
    "        return entity_id\n",
    "\n",
    "PROPERTY_LABELS = {pid: get_property_label(pid) for pid in property_ids}\n",
    "ENTITY_LABELS = {eid: get_entity_label(eid) for eid in entity_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba472542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_question(question):\n",
    "    question = re.sub(\n",
    "        r'http://www\\.wikidata\\.org/prop/direct/(p\\d+)',lambda m: m.group(1).upper(),question,flags=re.IGNORECASE)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f5fc0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question, property_labels):\n",
    "    question = clean_question(question)\n",
    "    for prop_id, label in property_labels.items():\n",
    "        question = question.replace(prop_id, label)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a98a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_by_id(entity_id, nodes=None):\n",
    "    return ENTITY_LABELS.get(entity_id, entity_id)\n",
    "\n",
    "def extract_humanized_facts(graph, focus_entity=None, property_labels=None):\n",
    "    edges = graph.get(\"edges\", [])\n",
    "    facts = []\n",
    "    for edge in edges:\n",
    "        if focus_entity and edge[\"source\"] != focus_entity:\n",
    "            continue\n",
    "        source_label = get_label_by_id(edge[\"source\"])\n",
    "        target_label = get_label_by_id(edge[\"target\"])\n",
    "        relation_id = edge.get(\"relation_id\", \"\")\n",
    "        relation = property_labels.get(relation_id, edge.get(\"relation\", relation_id))\n",
    "        facts.append(f\"{source_label} {relation} {target_label}\")\n",
    "    return facts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42781efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_context(question, facts, property_labels):\n",
    "    cleaned_question = process_question(question, property_labels)\n",
    "    context = \"\\n\".join(facts)\n",
    "    return f\"Context:\\n{context}\\n\\nQuestion: {cleaned_question}\\nAnswer:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d84d46ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "Donald Trump relative John G. Trump\n",
      "Donald Trump relative Vanessa Trump\n",
      "Donald Trump relative Jared Kushner\n",
      "Donald Trump relative Donald Trump III\n",
      "Donald Trump relative Elizabeth Christ Trump\n",
      "Donald Trump relative Lara Trump\n",
      "Donald Trump relative Mary L. Trump\n",
      "Donald Trump relative John Whitney Walter\n",
      "Donald Trump medical condition COVID-19\n",
      "Donald Trump topic's main Wikimedia portal Portal:Donald J. Trump\n",
      "\n",
      "Question: Which person serves as the head of state of United States?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# exmaple question + context \n",
    "sample = data[4]  \n",
    "\n",
    "leader_id = sample.get(\"leader_id\")  \n",
    "facts = extract_humanized_facts(sample[\"graph\"], focus_entity=leader_id, property_labels=PROPERTY_LABELS)\n",
    "\n",
    "prompt = build_prompt_with_context(sample[\"question\"], facts, PROPERTY_LABELS)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e26e09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(prompt, model, tokenizer, max_tokens=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Answer:\" in decoded:\n",
    "        response = decoded.split(\"Answer:\")[-1].strip()\n",
    "        return response.split(\"Question:\")[0].strip()\n",
    "    return decoded.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "42d4b1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "Donald Trump relative John G. Trump\n",
      "Donald Trump relative Vanessa Trump\n",
      "Donald Trump relative Jared Kushner\n",
      "Donald Trump relative Donald Trump III\n",
      "Donald Trump relative Elizabeth Christ Trump\n",
      "Donald Trump relative Lara Trump\n",
      "Donald Trump relative Mary L. Trump\n",
      "Donald Trump relative John Whitney Walter\n",
      "Donald Trump medical condition COVID-19\n",
      "Donald Trump topic's main Wikimedia portal Portal:Donald J. Trump\n",
      "\n",
      "Question: Which person serves as the head of state of United States?\n",
      "Answer:\n",
      "Model answer: The White House\n",
      "\n",
      "Question:\n",
      "Ground truth: Donald Trump\n"
     ]
    }
   ],
   "source": [
    "answer = query_llm(prompt, LLM, Tok)\n",
    "print(prompt)\n",
    "print(\"Model answer:\", answer)\n",
    "print(\"Ground truth:\", sample[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "364dcc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_context_given_instruction(question, facts, property_labels):\n",
    "    cleaned_question = process_question(question, property_labels)\n",
    "    context = \"\\n\".join(facts)\n",
    "    return (\"The context provided contains relevant facts. Stick to them when answering.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion: {cleaned_question}\\nAnswer:\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "56b3560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provided contains relevant facts. Stick to them when answering.\n",
      "\n",
      "Context:\n",
      "Donald Trump relative John G. Trump\n",
      "Donald Trump relative Vanessa Trump\n",
      "Donald Trump relative Jared Kushner\n",
      "Donald Trump relative Donald Trump III\n",
      "Donald Trump relative Elizabeth Christ Trump\n",
      "Donald Trump relative Lara Trump\n",
      "Donald Trump relative Mary L. Trump\n",
      "Donald Trump relative John Whitney Walter\n",
      "Donald Trump medical condition COVID-19\n",
      "Donald Trump topic's main Wikimedia portal Portal:Donald J. Trump\n",
      "\n",
      "Question: Which person serves as the head of state of United States?\n",
      "Answer:\n",
      "Model answer: Donald Trump\n",
      "Ground truth: Donald Trump\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt_with_context_given_instruction(sample[\"question\"], facts, PROPERTY_LABELS)\n",
    "answer = query_llm(prompt, LLM, Tok)\n",
    "print(prompt)\n",
    "print(\"Model answer:\", answer)\n",
    "print(\"Ground truth:\", sample[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
