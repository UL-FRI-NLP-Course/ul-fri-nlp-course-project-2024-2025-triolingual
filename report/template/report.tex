%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}
\usepackage{xcolor}
\definecolor{blue}{RGB}{0,0,139}

\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2025}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Integrating Structured Knowledge into Large Language Models} 

% Authors (student competitors) and their info
\Authors{Evgenii Posashkov, Biljana Vitanova, and Žan Štuhec}

% Advisors
\affiliation{\textit{Advisors: Slavko Žitnik}}

% Keywords
\Keywords{Largle Language Models, Knowledge Graphs, Question Answering, Reasoning}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
Large Language Models (LLMs) have demonstrated significant success in natural language processing tasks but still face challenges, one of which is handling complex question answering. This project explores methods to integrate structured knowledge, in form of knowledge graphs (KGs), into LLMs to enhance their performance. We aim to evaluate different techniques for integrating KGs. The ultimate goal is to assess whether the inclusion of structured knowledge can address the existing limitations of LLMs, and improve their reasoning capabilities. 

}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 
% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}


Despite advances in text generation by LLMs, they struggle with complex reasoning and fact recall. One approach to overcoming these limitations is integrating Knowledge Graphs (KGs), which provide structured representations of entities and relationships.  

Our project explores methods to incorporate KGs into LLMs by pairing questions with relevant KGs, bridging the gap between unstructured text and structured knowledge, enabling LLMs to access and utilize detailed information for advanced reasoning and accurate conclusions.

By using both textual and structural elements of KGs, we aim to improve the accuracy of LLMs in complex question-answering scenarios.




%------------------------------------------------
\section*{Related Work}
Recent research has explored multiple ways to integrate KGs into LLMs to better reasoning and improve task performance. Tang et al. (2024) introduced GraphGPT \cite{tang2024graphgpt}, which pairs LLMs with graph structures, aligning closely with our goal of improving question answering through KG integration. Kau et al. (2024) examined how KGs can mitigate issues like hallucinations in LLMs \cite{kau2024combining}, which is relevant to our focus on improving factual accuracy. Yao et al. (2019) developed KG-BERT for knowledge graph completion \cite{yao2019kg}, inspiring our use of graph embeddings in LLMs for better performance in complex reasoning tasks. Yasunaga et al. (2022) proposed DRAGON, a model that fuses text and KG during pretraining \cite{yasunaga2022deep}, which relates to our aim of combining textual and structural elements of KGs to enhance LLM capabilities. Kang et al. (2023) presented SURGE for knowledge-grounded dialogue \cite{kang2023knowledge}, which aligns with our goal of ensuring that KGs improve LLMs factual accuracy in more complex tasks. And in the end Wang et al. (2017) surveyed KG embedding techniques \cite{wang2017knowledge}, providing the basis for our exploration of graph embeddings like TransE and node2vec to better LLM performance. These studies together inform our methodology for integrating KGs into LLMs to improve their reasoning and accuracy in complex question answering.
\section*{Representation and Integration of KGs}

KGs and LLMs can be integrated in two ways: KG-enhanced LLMs, where knowledge graphs improve the responses of large language models, and LLM-enhanced KGs, where language models enhance the representation of knowledge graphs \cite{Pan2024UnifyingLLMKG}. In our project, we will focus on the first approach—KG-enhanced LLMs, or integration of KGs in LLMs.



 
 KGs can be integrated into LLMs at different stages on model's development. Based on when the integration occurs, we can distinguish between two types of KG enhancement.  


\begin{enumerate}
    \item During-training enhancement: KGs are integrated into the model’s training process, modifying the model's architecture to recognize this structured information.
    \item Post-training enhancement: After the model has been trained, KGs are used to refine the performance by providing additional information through the prompt itself.
\end{enumerate}



Additionally, Knowledge Graphs can be classified based on the type of knowledge they represent:
 \begin{enumerate}
     \item Encyclopedic KGs – These KGs store broad real-world knowledge, providing general information. Examples include Wikidata, NELL, DBpedia.
     \item Commonsense KGs – which capture logical relationships from everyday life. Examples are: ConceptNet, ATOMIC. 
     \item Domain-Specific KGs – Focus on specialized fields like medicine or finance. One example is UMSL which is used in the mdecial field. 
     \item Multimodal KGs – Combine structured data with multiple sources, such as images and text. IMGpedia is an example of such.
 \end{enumerate}
 





\section*{Methodology}

In the following section, we will introduce our primary plan on how we aim to approach this project.
There are four key questions we need to answer: Which integration techniques will be applied, which LLMs will be used as baselines, how will the evaluation dataset be created, and how will the evaluation be performed.

\subsection*{Knowledge Graph Integration Techniques}


In our implementation, our main focus will be on during-training enhancement, as we expect it to yield better performance. To achieve this, we will experiment with different graph embeddings such as TransE, LapEigen, node2vec, and DeepWalk, and analyze their performance.
We will explore integrating attention-based fusion techniques, where the model uses its attention mechanism to focus on the most relevant parts of the knowledge graph, allowing it to dynamically combine structured knowledge with textual data for improved predictions

\subsection*{Baseline LLMs}


As baselines we will use already pretrained models like GPT, BERT, T-5 available online. 

\subsection*{Evaluation Dataset}

For the final work, we plan to use Slovenian linguistic data from the "Slovenian Digital Dictionary Database". As we do not have access to this data yet, our starting point will be datasets that contain complex questions \cite{talmor2018web},\cite{zhang2017variational}, which require different levels of reasoning. For example: 
\begin{enumerate}
    \item one-hoop: What movies are about [George Pal]? - 
    \item two-hoop: What movies are about [a director who won an Oscar]? (two-hop)
    \item three-hoop: Which person wrote the films directed by the director of [Four Brothers]? 
\end{enumerate}



Questions will be paired with a relevant knowledge graphs extracted from sources that provide structured data, such as DBpedia, Wikidata or similar databases. 
Additionally, we will explore how to pair multiple knowledge graphs and what makes a well-structured knowledge graph.



\subsection*{Performance Evaluation}

Performance evaluation will be done from several aspects. First, we’ll check if the answers are correct and relevant. Then, we’ll assess how well the model handles questions of different complexity, especially those requiring multi-step reasoning. We’ll also measure response time to see how quickly the model generates answers. Finally, we’ll evaluate how well the model uses structured data from knowledge graphs to improve its answers.


%------------------------------------------------

%\section*{Results}

%se the results section to present the final results of your work. Present the results in a objective and scientific fashion. Use visualisations to convey your results in a clear and efficient manner. When comparing results between various techniques use appropriate statistical methodology.





%------------------------------------------------

\section*{Discussion}
We acknowledge that some aspects of the methodology may not be fully accurate at this stage, as our focus has been primarily on theoretical understanding. Through implementation, we expect to gain further insights and be able to provide a more detailed and refined explanation.



%------------------------------------------------

% \section*{Acknowledgments}



%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}